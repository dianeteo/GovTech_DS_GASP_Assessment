{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b3f484",
   "metadata": {},
   "source": [
    "# 2. Proposed Approaches for Classification/Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1cba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98da6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitment_df_processed = pd.read_csv('recruitment_df_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba2b00",
   "metadata": {},
   "source": [
    "As stated earlier, the dataset is a highly imbalanced dataset with only 5% of observations being tagged as fraudulent. Given this imbalance, we deal with it at the model level using class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2707c6",
   "metadata": {},
   "source": [
    "## 2A. Classification Models with Features and Empirical Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4273c44",
   "metadata": {},
   "source": [
    "The baseline model for this classification/fraud detection model would be to use the features given in the dataset together with a list of empirical rules to train a baseline classification model, specifically a Logistic Regression model.\n",
    "\n",
    "Given that through EDA, we found out that given features such as required experience, required education and whether the job advertisement had a company profile/logo/screening questions were different between fraudulent job advertisements and non-fraudulent ones, the natural step would be to use these features as regressors. We also found out that derived features such as the length of company profile, description and requirements, together with other signals like money in description, money in title, had significant differences between fraudulent and non-fraudulent job advertisements, so we include these features as well in the logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3110e5",
   "metadata": {},
   "source": [
    "### Preparing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846c020",
   "metadata": {},
   "source": [
    "List of features used:\n",
    "1. Binary Features: `has_company_profile`, `has_company_logo`, `has_questions`, `money_in_desc`, `money_in_title`, `url_in_description`, `consecutive_punct`\n",
    "2. Categorical Features: `employment_type`, `required_experience`, `required_education`\n",
    "3. Numerical Features: `company_profile_len`, `description_len`, `requirements_len`\n",
    "\n",
    "These features have been generated above in the EDA stage and the code below prepares the features for model training (e.g. converting all binary features to '1's and '0's)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b736a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = recruitment_df_processed.copy()\n",
    "\n",
    "df[\"fraud_target\"] = df[\"fraudulent\"].map({'t':1, 'f':0}).astype(int)\n",
    "\n",
    "# --- Feature groups ---\n",
    "bin_cols = [\"has_company_profile\", \"has_company_logo\", \"has_questions\", \n",
    "            \"money_in_desc\", \"money_in_title\", \n",
    "            \"url_in_description\", \"consecutive_punct\"]\n",
    "\n",
    "cat_cols = [\"employment_type\", \"required_experience\", \"required_education\"]\n",
    "\n",
    "num_cols = [\"company_profile_stripped_len\", \"description_stripped_len\", \"requirements_stripped_len\"]\n",
    "\n",
    "# --- Binary features: coerce to {0,1} ---\n",
    "to01 = {\n",
    "    True: 1, False: 0,\n",
    "    't': 1, 'f': 0, 'T': 1, 'F': 0,\n",
    "    'true': 1, 'false': 0, 'TRUE': 1, 'FALSE': 0,\n",
    "    '1': 1, '0': 0,\n",
    "    1: 1, 0: 0\n",
    "}\n",
    "for b in bin_cols:\n",
    "    df[b] = df[b].map(to01)  # map known representations\n",
    "    df[b] = df[b].fillna(0).astype(int)  # default missing â†’ 0\n",
    "\n",
    "# --- Categorical features: fill NAs with \"Unknown\" ---\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].fillna(\"Unknown\")\n",
    "\n",
    "# --- Target ---\n",
    "y = df[\"fraud_target\"]\n",
    "\n",
    "# --- Preprocessing pipeline ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"bin\", \"passthrough\", bin_cols),  # already 0/1\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(class_weight=\"balanced\", max_iter=1000))\n",
    "])\n",
    "\n",
    "# --- Train/test split (if training) ---\n",
    "X = df[bin_cols + cat_cols + num_cols]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3013e2",
   "metadata": {},
   "source": [
    "## 2B. TF-IDF and Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc57754",
   "metadata": {},
   "source": [
    "While the previous classification model uses metadata features, term frequency-inverse document frequency captures the nuance of the job description, title, company profile and requirements. TF-IDF is a simplistic way of allowing the model learn patterns from the writeups before proceeding to more advanced methods like word embeddings and using LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a6c23",
   "metadata": {},
   "source": [
    "### Preparing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3a322",
   "metadata": {},
   "source": [
    "We have previously used BeautifulSoup to clean up the job description and here we also build a composite text where we append the job title, company profile, description and requirements together before fitting the term frequency-inverse document frequency transformation. We combine these text so that the logistic regression model can treat the entire advertisement as a single document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db98002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build composite text (title + profile + description + requirements) ---\n",
    "text_cols = [\"title\", \"company_profile_stripped\", \"description_stripped\", \"requirements_stripped\"]\n",
    "\n",
    "df[\"text_all\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "# (Optional) light cleanup: collapse whitespace\n",
    "df[\"text_all\"] = df[\"text_all\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "X_text = df[\"text_all\"]\n",
    "y = df[\"fraud_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Train/test split (stratified) ---\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# # --- TF-IDF + class-weighted Logistic Regression pipeline ---\n",
    "# tfidf_lr = Pipeline([\n",
    "#     (\"tfidf\", TfidfVectorizer(\n",
    "#         lowercase=True,\n",
    "#         stop_words=\"english\",    # remove common stopwords\n",
    "#         ngram_range=(1, 2),      # unigrams + bigrams catch short phrases\n",
    "#         max_df=0.9,              # drop terms in >90% of docs (too common)\n",
    "#         min_df=5,                # keep terms that appear in at least 5 docs\n",
    "#         max_features=50000       # cap dimensionality (tune as needed)\n",
    "#     )),\n",
    "#     (\"clf\", LogisticRegression(\n",
    "#         class_weight=\"balanced\", # handle class imbalance at the model level\n",
    "#         max_iter=2000,\n",
    "#         solver=\"liblinear\"       # good default for sparse features\n",
    "#         # try 'saga' if you want L1/elasticnet regularization\n",
    "#     ))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fba68",
   "metadata": {},
   "source": [
    "## 2C. Combination of TF-IDF, Features and Empirical Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b584c715",
   "metadata": {},
   "source": [
    "## 2D. Word Embeddings and LLMs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
